{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgde1MeD39yk",
        "outputId": "aebc7511-cf79-4ecd-d2ca-29e186ab6dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision transformers\n",
        "#load pretrained bert base model\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from sklearn.metrics import f1_score,  precision_score, recall_score\n",
        "import numpy as np \n",
        "import time\n",
        "import xgboost\n",
        "\n",
        "\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "#load BERT's WordPiece tokenisation model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGtUWjhr39zC"
      },
      "outputs": [],
      "source": [
        "class SSTDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, maxlen):\n",
        "\n",
        "        #Store the contents of the file in a pandas dataframe\n",
        "        self.df = pd.read_pickle(filename)\n",
        "\n",
        "        #Initialize the BERT tokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        #Selecting the sentence and label at the specified index in the data frame\n",
        "        tweet_text = self.df.loc[index, 'text']\n",
        "        reply_text = self.df.loc[index, 'reply_text']\n",
        "        followers_count = self.df.loc[index, 'followers_count']\n",
        "        if 'label' in self.df.columns:\n",
        "          label = self.df.loc[index, 'label']\n",
        "        #other_features = np.array([self.df.loc[index, ['followers_count']]])\n",
        "        other_features = torch.tensor(self.df.loc[index, ['followers_count', 'friends_count', 'listed_count', 'verified','is_reply','favourites_count','retweet_count','favorite_count', 'reply_avg_sent']])\n",
        "        #Preprocessing the text to be suitable for BERT\n",
        "        tokens = self.tokenizer.tokenize(tweet_text+' '+reply_text) #Tokenize the sentence\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n",
        "        if len(tokens) < self.maxlen:\n",
        "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
        "        else:\n",
        "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
        "\n",
        "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
        "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
        "\n",
        "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
        "        attn_mask = (tokens_ids_tensor != 0).long()\n",
        "        if 'label' in self.df.columns:\n",
        "          return tokens_ids_tensor, attn_mask,other_features, label\n",
        "        else:\n",
        "          return tokens_ids_tensor, attn_mask,other_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzhUQHT439zF",
        "outputId": "904b58b7-8196-4c72-f732-6ed5e27e93bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done preprocessing training and development data.\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#Creating instances of training and development set\n",
        "#maxlen sets the maximum length a sentence can have\n",
        "#any sentence longer than this length is truncated to the maxlen size\n",
        "train_set = SSTDataset(filename = './train_df.pkl', maxlen = 256)\n",
        "dev_set = SSTDataset(filename = './dev_df.pkl', maxlen = 256)\n",
        "test_set = SSTDataset(filename = './test_df.pkl', maxlen = 256)\n",
        "\n",
        "#Creating intsances of training and development dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size = 32, num_workers = 2)\n",
        "dev_loader = DataLoader(dev_set, batch_size = 32, num_workers = 2)\n",
        "test_loader = DataLoader(test_set, batch_size = 32, num_workers = 2)\n",
        "\n",
        "print(\"Done preprocessing training and development data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mC5rpX939zI"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RumorDetector(nn.8):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(RumorDetector, self).__init__()\n",
        "        #Instantiating BERT model object \n",
        "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        #Classification layer\n",
        "        #input dimension is 768 because [CLS] embedding has a dimension of 768\n",
        "        #output dimension is 1 because we're working with a binary classification problem\n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "        #self.cls_layer = nn.Linear(1033, 1)\n",
        "\n",
        "    def forward(self, seq, attn_masks, other_features):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "\n",
        "        #Feeding the input to BERT model to obtain contextualized representations\n",
        "        outputs = self.bert_layer(seq, attention_mask = attn_masks)\n",
        "        cont_reps = outputs.last_hidden_state\n",
        "        #Obtaining the representation of [CLS] head (the first token)\n",
        "        #print(cont_reps[:, 0].shape)\n",
        "        #print(other_features.shape)\n",
        "        #cls_rep = torch.cat([cont_reps[:, 0], other_features],1).float()\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "        #Feeding cls_rep to the classifier layer\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuT4iQWN39zK",
        "outputId": "8285de10-1840-44ed-d169-5b5493714d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RumorDetector(\n",
              "  (bert_layer): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (cls_layer): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "gpu = 0 #gpu ID\n",
        "\n",
        "net = RumorDetector()\n",
        "net.cuda(gpu) #Enable gpu support for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEFvInHa39zN"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.Adam(net.parameters(), lr = 2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZlOKYq339zP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
        "\n",
        "    best_f1 = 0\n",
        "    st = time.time()\n",
        "    for ep in range(max_eps):\n",
        "        \n",
        "        net.train()\n",
        "        for it, (seq, attn_masks, other_features, labels) in enumerate(train_loader):\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Converting these to cuda tensors\n",
        "            seq, attn_masks, other_features, labels = seq.cuda(gpu), attn_masks.cuda(gpu), other_features.cuda(gpu), labels.cuda(gpu)\n",
        "            #Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks, other_features)\n",
        "            #Computing loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "            #Backpropagating the gradients\n",
        "            loss.backward()\n",
        "            #Optimization step\n",
        "            opti.step()\n",
        "              \n",
        "            if it % 10 == 0:\n",
        "                acc = get_accuracy_from_logits(logits, labels)\n",
        "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
        "                st = time.time()\n",
        "\n",
        "        dev_acc, dev_loss, f1, p_score, r_score = evaluate(net, criterion, dev_loader, gpu)\n",
        "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}, Recall: {}, Precision: {} and F1 Score: {}\".format(ep, dev_acc, dev_loss, r_score, p_score, f1))\n",
        "        if f1 > best_f1:\n",
        "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_f1, f1))\n",
        "            best_f1 = f1\n",
        "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2qR1DKQ39zS"
      },
      "outputs": [],
      "source": [
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "def evaluate(net, criterion, dataloader, gpu):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "    targets = []\n",
        "    outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, other_features, labels in dataloader:\n",
        "            seq, attn_masks, other_features, labels = seq.cuda(gpu), attn_masks.cuda(gpu),other_features.cuda(gpu), labels.cuda(gpu)\n",
        "            logits = net(seq, attn_masks, other_features)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "            count += 1\n",
        "            outputs.append(logits.detach().cpu().numpy())\n",
        "            targets.append(labels.cpu().numpy())\n",
        "        outputs = np.concatenate(outputs)\n",
        "        targets = np.concatenate(targets)\n",
        "        f1 = f1_score(targets,[1 if sigmoid(x)>0.5 else 0 for x in outputs])\n",
        "        p_score = precision_score(targets,[1 if sigmoid(x)>0.5 else 0 for x in outputs])\n",
        "        r_score = recall_score(targets,[1 if sigmoid(x)>0.5 else 0 for x in outputs])\n",
        "    return mean_acc / count, mean_loss / count, f1, p_score, r_score\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def predict(model, dataloader, gpu):\n",
        "    model.eval()\n",
        "\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "      for seq, attn_masks, other_features in dataloader:\n",
        "        seq, attn_masks, other_features = seq.cuda(gpu), attn_masks.cuda(gpu), other_features.cuda(gpu)\n",
        "        logits = model(seq, attn_masks, other_features)\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        for logit in logits:\n",
        "            preds.append(sigmoid(logit))\n",
        "\n",
        "    return preds\n",
        "\n",
        "def extract_embedding(model, dataloader, gpu):\n",
        "    model.eval()\n",
        "\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "      for seq, attn_masks, other_features, label in dataloader:\n",
        "        seq, attn_masks, other_features = seq.cuda(gpu), attn_masks.cuda(gpu), other_features.cuda(gpu)\n",
        "        logits = model(seq, attn_masks, other_features)\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        for logit in logits:\n",
        "            preds.append(sigmoid(logit))\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grPQtazw39zV",
        "outputId": "ef2748ee-fc34-4521-bd28-1680e5d7dcca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 of epoch 0 complete. Loss: 0.6569644212722778; Accuracy: 0.65625; Time taken (s): 3.5441129207611084\n",
            "Iteration 10 of epoch 0 complete. Loss: 0.4777846038341522; Accuracy: 0.78125; Time taken (s): 25.867339611053467\n",
            "Iteration 20 of epoch 0 complete. Loss: 0.22273902595043182; Accuracy: 0.9375; Time taken (s): 25.87035298347473\n",
            "Iteration 30 of epoch 0 complete. Loss: 0.36775755882263184; Accuracy: 0.75; Time taken (s): 25.81198763847351\n",
            "Iteration 40 of epoch 0 complete. Loss: 0.3795575499534607; Accuracy: 0.875; Time taken (s): 25.786431074142456\n",
            "Epoch 0 complete! Development Accuracy: 0.810735285282135; Development Loss: 0.36984142310479107, Recall: 0.5826086956521739, Precision: 0.5583333333333333 and F1 Score: 0.5702127659574469\n",
            "Best development accuracy improved from 0 to 0.810735285282135, saving model...\n",
            "Iteration 0 of epoch 1 complete. Loss: 0.3734000325202942; Accuracy: 0.875; Time taken (s): 41.2684109210968\n",
            "Iteration 10 of epoch 1 complete. Loss: 0.2897026836872101; Accuracy: 0.84375; Time taken (s): 25.761420965194702\n",
            "Iteration 20 of epoch 1 complete. Loss: 0.1174447238445282; Accuracy: 0.9375; Time taken (s): 25.75594186782837\n",
            "Iteration 30 of epoch 1 complete. Loss: 0.3096291422843933; Accuracy: 0.84375; Time taken (s): 25.744466543197632\n",
            "Iteration 40 of epoch 1 complete. Loss: 0.14661143720149994; Accuracy: 0.96875; Time taken (s): 25.71726655960083\n",
            "Epoch 1 complete! Development Accuracy: 0.8831617832183838; Development Loss: 0.2405738865627962, Recall: 0.5130434782608696, Precision: 0.8939393939393939 and F1 Score: 0.6519337016574586\n",
            "Best development accuracy improved from 0.810735285282135 to 0.8831617832183838, saving model...\n",
            "Iteration 0 of epoch 2 complete. Loss: 0.25322219729423523; Accuracy: 0.875; Time taken (s): 41.510353803634644\n",
            "Iteration 10 of epoch 2 complete. Loss: 0.15069660544395447; Accuracy: 0.90625; Time taken (s): 25.842961072921753\n",
            "Iteration 20 of epoch 2 complete. Loss: 0.016619697213172913; Accuracy: 1.0; Time taken (s): 25.79056692123413\n",
            "Iteration 30 of epoch 2 complete. Loss: 0.06934931874275208; Accuracy: 0.96875; Time taken (s): 25.779442071914673\n",
            "Iteration 40 of epoch 2 complete. Loss: 0.014790154062211514; Accuracy: 1.0; Time taken (s): 25.743388891220093\n",
            "Epoch 2 complete! Development Accuracy: 0.9503676295280457; Development Loss: 0.14141967454377344, Recall: 0.9043478260869565, Precision: 0.8666666666666667 and F1 Score: 0.8851063829787233\n",
            "Best development accuracy improved from 0.8831617832183838 to 0.9503676295280457, saving model...\n",
            "Iteration 0 of epoch 3 complete. Loss: 0.06036433205008507; Accuracy: 0.96875; Time taken (s): 41.388829946517944\n",
            "Iteration 10 of epoch 3 complete. Loss: 0.04649941995739937; Accuracy: 0.96875; Time taken (s): 25.755725622177124\n",
            "Iteration 20 of epoch 3 complete. Loss: 0.0097776148468256; Accuracy: 1.0; Time taken (s): 25.71553611755371\n",
            "Iteration 30 of epoch 3 complete. Loss: 0.016970496624708176; Accuracy: 1.0; Time taken (s): 25.726359844207764\n",
            "Iteration 40 of epoch 3 complete. Loss: 0.03629498928785324; Accuracy: 0.96875; Time taken (s): 25.680516719818115\n",
            "Epoch 3 complete! Development Accuracy: 0.9388235211372375; Development Loss: 0.19628486754920552, Recall: 0.9565217391304348, Precision: 0.7971014492753623 and F1 Score: 0.8695652173913043\n",
            "Iteration 0 of epoch 4 complete. Loss: 0.013429101556539536; Accuracy: 1.0; Time taken (s): 40.06839561462402\n",
            "Iteration 10 of epoch 4 complete. Loss: 0.018041856586933136; Accuracy: 1.0; Time taken (s): 25.77886462211609\n",
            "Iteration 20 of epoch 4 complete. Loss: 0.01667940802872181; Accuracy: 1.0; Time taken (s): 25.708845615386963\n",
            "Iteration 30 of epoch 4 complete. Loss: 0.004034573212265968; Accuracy: 1.0; Time taken (s): 25.737499237060547\n",
            "Iteration 40 of epoch 4 complete. Loss: 0.10667860507965088; Accuracy: 0.96875; Time taken (s): 25.77932858467102\n",
            "Epoch 4 complete! Development Accuracy: 0.8385294079780579; Development Loss: 0.5787489177549586, Recall: 0.991304347826087, Precision: 0.57 and F1 Score: 0.7238095238095238\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 5\n",
        "\n",
        "for x in os.listdir('.'):\n",
        "  if x.startswith('sstcls'):\n",
        "    os.remove(x)\n",
        "\n",
        "#fine-tune the model\n",
        "train(net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "latest_ep = max([int(re.match(r'sstcls_([0-9]).dat', x)[1]) for x in os.listdir('.') if x.startswith('sstcls')])\n",
        "net.load_state_dict(torch.load(f'./sstcls_{latest_ep}.dat'))\n",
        "p = predict(net, test_loader, gpu)\n",
        "df = pd.DataFrame([1 if x>0.5 else 0 for x in p], columns =['Predicted'])\n",
        "df.to_csv('test_Submission.csv', index_label='Id')\n"
      ],
      "metadata": {
        "id": "ozBDQZlgDm_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_pickle('./train_df.pkl')\n",
        "train_embed = extract_embedding(net, train_loader, gpu)\n",
        "train_df['embed'] = train_embed\n",
        "\n",
        "dev_df = pd.read_pickle('./dev_df.pkl')\n",
        "dev_embed = extract_embedding(net, dev_loader, gpu)\n",
        "dev_df['embed'] = dev_embed"
      ],
      "metadata": {
        "id": "2-yiH0EtB-Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[['followers_count', 'friends_count', 'listed_count', 'verified','is_reply','favourites_count','retweet_count','favorite_count', 'reply_avg_sent','train_embed']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "XoPMXuEbyArL",
        "outputId": "27ba0bc7-3423-4963-ea34-64bf09d61ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      followers_count  friends_count  listed_count  verified  is_reply  \\\n",
              "0           -0.329117      -0.046457     -0.376736         0         1   \n",
              "1            0.065073      -0.066977      0.276132         1         0   \n",
              "2           -0.329092      -0.066042     -0.376736         0         0   \n",
              "3            2.081211      -0.064422      3.574830         1         0   \n",
              "4           -0.328566      -0.007887     -0.376546         0         0   \n",
              "...               ...            ...           ...       ...       ...   \n",
              "1561        -0.329089      -0.070369     -0.376736         0         1   \n",
              "1562        -0.311690       0.740538     -0.336669         1         0   \n",
              "1563         1.800456      -0.072477      1.689071         1         0   \n",
              "1564         0.778721      -0.053339      1.217261         1         0   \n",
              "1565        -0.328787      -0.052724     -0.375505         0         1   \n",
              "\n",
              "      favourites_count  retweet_count  favorite_count  reply_avg_sent  \\\n",
              "0             0.239585      -0.349946       -0.263323       -0.368300   \n",
              "1            -0.398676       0.158194       -0.154328       -1.730077   \n",
              "2             0.075617      -0.347184       -0.253414       -0.489189   \n",
              "3            -0.383882      -0.018550       -0.206348       -0.501423   \n",
              "4             0.008665      -0.347184       -0.248460       -0.035306   \n",
              "...                ...            ...             ...             ...   \n",
              "1561          1.228920      -0.349946       -0.263323        0.045623   \n",
              "1562          0.739320      -0.140062       -0.124602       -1.097507   \n",
              "1563         -0.404951      -0.048928        0.113206       -0.295895   \n",
              "1564         -0.390956       0.138863       -0.107261       -1.445863   \n",
              "1565         -0.257432      -0.349946       -0.263323        1.163682   \n",
              "\n",
              "      train_embed  \n",
              "0        0.000924  \n",
              "1        0.901331  \n",
              "2        0.001571  \n",
              "3        0.019478  \n",
              "4        0.003655  \n",
              "...           ...  \n",
              "1561     0.001561  \n",
              "1562     0.994374  \n",
              "1563     0.968023  \n",
              "1564     0.014335  \n",
              "1565     0.005057  \n",
              "\n",
              "[1566 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3dd70bdf-f561-4e6e-880e-d94d2230b682\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>followers_count</th>\n",
              "      <th>friends_count</th>\n",
              "      <th>listed_count</th>\n",
              "      <th>verified</th>\n",
              "      <th>is_reply</th>\n",
              "      <th>favourites_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>reply_avg_sent</th>\n",
              "      <th>train_embed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.329117</td>\n",
              "      <td>-0.046457</td>\n",
              "      <td>-0.376736</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.239585</td>\n",
              "      <td>-0.349946</td>\n",
              "      <td>-0.263323</td>\n",
              "      <td>-0.368300</td>\n",
              "      <td>0.000924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.065073</td>\n",
              "      <td>-0.066977</td>\n",
              "      <td>0.276132</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.398676</td>\n",
              "      <td>0.158194</td>\n",
              "      <td>-0.154328</td>\n",
              "      <td>-1.730077</td>\n",
              "      <td>0.901331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.329092</td>\n",
              "      <td>-0.066042</td>\n",
              "      <td>-0.376736</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.075617</td>\n",
              "      <td>-0.347184</td>\n",
              "      <td>-0.253414</td>\n",
              "      <td>-0.489189</td>\n",
              "      <td>0.001571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.081211</td>\n",
              "      <td>-0.064422</td>\n",
              "      <td>3.574830</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.383882</td>\n",
              "      <td>-0.018550</td>\n",
              "      <td>-0.206348</td>\n",
              "      <td>-0.501423</td>\n",
              "      <td>0.019478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.328566</td>\n",
              "      <td>-0.007887</td>\n",
              "      <td>-0.376546</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.008665</td>\n",
              "      <td>-0.347184</td>\n",
              "      <td>-0.248460</td>\n",
              "      <td>-0.035306</td>\n",
              "      <td>0.003655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1561</th>\n",
              "      <td>-0.329089</td>\n",
              "      <td>-0.070369</td>\n",
              "      <td>-0.376736</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.228920</td>\n",
              "      <td>-0.349946</td>\n",
              "      <td>-0.263323</td>\n",
              "      <td>0.045623</td>\n",
              "      <td>0.001561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1562</th>\n",
              "      <td>-0.311690</td>\n",
              "      <td>0.740538</td>\n",
              "      <td>-0.336669</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.739320</td>\n",
              "      <td>-0.140062</td>\n",
              "      <td>-0.124602</td>\n",
              "      <td>-1.097507</td>\n",
              "      <td>0.994374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1563</th>\n",
              "      <td>1.800456</td>\n",
              "      <td>-0.072477</td>\n",
              "      <td>1.689071</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.404951</td>\n",
              "      <td>-0.048928</td>\n",
              "      <td>0.113206</td>\n",
              "      <td>-0.295895</td>\n",
              "      <td>0.968023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1564</th>\n",
              "      <td>0.778721</td>\n",
              "      <td>-0.053339</td>\n",
              "      <td>1.217261</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.390956</td>\n",
              "      <td>0.138863</td>\n",
              "      <td>-0.107261</td>\n",
              "      <td>-1.445863</td>\n",
              "      <td>0.014335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1565</th>\n",
              "      <td>-0.328787</td>\n",
              "      <td>-0.052724</td>\n",
              "      <td>-0.375505</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.257432</td>\n",
              "      <td>-0.349946</td>\n",
              "      <td>-0.263323</td>\n",
              "      <td>1.163682</td>\n",
              "      <td>0.005057</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1566 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dd70bdf-f561-4e6e-880e-d94d2230b682')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3dd70bdf-f561-4e6e-880e-d94d2230b682 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3dd70bdf-f561-4e6e-880e-d94d2230b682');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting grid of selected parameters for iteration\n",
        "params = {'gamma': [0,0.1,0.4,0.8,1.6],\n",
        "              'learning_rate': [0.01, 0.03, 0.1, 0.15, 0.5],\n",
        "              'max_depth': [5,7,10],\n",
        "              'n_estimators': [50,100,150],\n",
        "              'reg_alpha': [0,0.1,0.4,0.8,1.6],\n",
        "              'reg_lambda': [0,0.1,0.4,0.8,1.6]}\n",
        "\n",
        "\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "\n",
        "xgb_model = XGBClassifier()\n",
        "\n",
        "model = GridSearchCV(estimator = xgb_model,param_grid = params)\n",
        "model.fit(train_df[['followers_count', 'friends_count', 'listed_count', 'verified','is_reply','favourites_count','retweet_count','favorite_count', 'reply_avg_sent','embed']], train_df.label)\n",
        "model.best_params_"
      ],
      "metadata": {
        "id": "99tWlG43BD5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "clf = xgb.XGBClassifier(max_depth=200, n_estimators=400, subsample=1, learning_rate=0.07, reg_lambda=0.1, reg_alpha=0.1, gamma=1)\n",
        "clf.fit(train_df[['followers_count', 'friends_count', 'listed_count', 'verified','is_reply','favourites_count','retweet_count','favorite_count', 'reply_avg_sent','embed']], train_df.label)\n"
      ],
      "metadata": {
        "id": "z6zfVdu0BhLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca0e9d5-ceb5-4b9d-c9f3-d56791d1e4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(gamma=1, learning_rate=0.07, max_depth=200, n_estimators=400,\n",
              "              reg_alpha=0.1, reg_lambda=0.1)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predictions = clf.predict(dev_df[['followers_count', 'friends_count', 'listed_count', 'verified','is_reply','favourites_count','retweet_count','favorite_count', 'reply_avg_sent','embed']])\n",
        "print (\"Training set f1_score :\", np.round(f1_score(dev_df.label, predictions),5))"
      ],
      "metadata": {
        "id": "DAJhZkTFDhoP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd09328-553a-401b-8e05-eaab864df845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set f1_score : 0.88496\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Bert_XGBOOST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}